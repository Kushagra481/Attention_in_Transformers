{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VGoK_Dy_qRTh"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "u2HxfjtGqVjx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "        ## d_model = the number of embedding values per token.\n",
        "        ##           Because we want to be able to do the math by hand, we've\n",
        "        ##           the default value for d_model=2.\n",
        "        ##           However, in \"Attention Is All You Need\" d_model=512\n",
        "        ##\n",
        "        ## row_dim, col_dim = the indices we should use to access rows or columns\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## Initialize the Weights (W) that we'll use to create the\n",
        "        ## query (q), key (k) and value (v) for each token\n",
        "        ## NOTE: A lot of implementations include bias terms when\n",
        "        ##       creating the the queries, keys, and values, but\n",
        "        ##       the original manuscript that described Attention,\n",
        "        ##       \"Attention Is All You Need\" did not, so we won't either\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "\n",
        "    def forward(self, token_encodings):\n",
        "        ## Create the query, key and values using the encoding numbers\n",
        "        ## associated with each token (token encodings)\n",
        "        q = self.W_q(token_encodings)\n",
        "        k = self.W_k(token_encodings)\n",
        "        v = self.W_v(token_encodings)\n",
        "\n",
        "        ## Compute similarities scores: (q * k^T)\n",
        "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "\n",
        "        ## Scale the similarities by dividing by sqrt(k.col_dim)\n",
        "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "        ## Apply softmax to determine what percent of each tokens' value to\n",
        "        ## use in the final attention values.\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        ## Scale the values by their associated percentages and add them up.\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ],
      "metadata": {
        "id": "91E1hjEMspnX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create a matrix of token encodings...\n",
        "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])\n",
        "\n",
        "## set the seed for the random number generator\n",
        "torch.manual_seed(42)\n",
        "\n",
        "## create a basic self-attention ojbect\n",
        "selfAttention = SelfAttention(d_model=2,\n",
        "                               row_dim=0,\n",
        "                               col_dim=1)\n",
        "\n",
        "## calculate basic attention for the token encodings\n",
        "selfAttention(encodings_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd1WQVC5ruLn",
        "outputId": "83809bff-4178-43bc-89b2-c1321de84b0e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100, 1.0641],\n",
              "        [0.2040, 0.7057],\n",
              "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selfAttention.W_q.weight.transpose(0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnXxS0j_sHO-",
        "outputId": "14c88329-b01d-4f8f-c4e7-989a79008803"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5406, -0.1657],\n",
              "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selfAttention.W_k.weight.transpose(0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A4VJyC4s4BH",
        "outputId": "8926eac9-1141-4eff-932f-f251679bee96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1549, -0.3443],\n",
              "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selfAttention.W_v.weight.transpose(0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMHtQErOs5KF",
        "outputId": "75d2003b-884d-4d6f-d44d-1fba8aa3ff47"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6233,  0.6146],\n",
              "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "selfAttention.W_q(encodings_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_T7ndcCs6U3",
        "outputId": "482b34b4-92e4-4665-ec06-39c5733ca127"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7621, -0.0428],\n",
              "        [ 1.1063,  0.7890],\n",
              "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "selfAttention.W_k(encodings_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwcmv-2Bs7ka",
        "outputId": "252d3a0e-388e-42cc-d2f3-6f08f5da7ed8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1469, -0.3038],\n",
              "        [ 0.1057,  0.3685],\n",
              "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "selfAttention.W_v(encodings_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgMtAeOas8nj",
        "outputId": "9604d978-41f8-4ba8-ffd6-09a98c597633"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6038,  0.7434],\n",
              "        [-0.3502,  0.5303],\n",
              "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = selfAttention.W_k(encodings_matrix)\n",
        "k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBIx5qRzs9z3",
        "outputId": "1a230079-60f5-4eed-8c41-4fb4cc7bf9a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1469, -0.3038],\n",
              "        [ 0.1057,  0.3685],\n",
              "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-ov5wdLtCFb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}